## Retrieval Augmented Generation (RAG) with LangChain
This repository contains simple application examples for RAG. 

## Limitations of LLM
- **Knowledge Cutoff**: Unable to answer data generated after model training due to lack of learning
- **No access to private data**: No access to confidential, personal, and non-public information inside the company
- **Hallucinations**: Provide plausible and natural answers that are not based on facts
- **General Purpose**: Can answer more than a certain level in various fields, but there may be limitations in certain areas of expertise

  RAG method is widely used with Fine-Tuning as a way to solve these problems


## What's RAG?
- **Combining Retrieval and Generation**: The RAG model integrates a document search system with a seq2seq language model. The search system first imports relevant documents or information from a large corpus based on input queries.
- **Information Augmentation**: Retrieved documents are used to augment the input of language models. This provides additional context and information that the model did not learn during initial training. Thus, the model will be able to produce more accurate and contextual responses.
- **Applications**: RAG is particularly useful for question-answering systems, where models must provide specific, factual answers that are not included in the training data. It is also used in applications that require external data sources for accuracy or require technical responses.

  Implementing the RAG system typically involves combining search models such as Elasticsearch or Dense Vector Search System with seq2seq2 language models such as GPT or BART.

## Methods and procedures

1) Convert documents to text embeddings and store them in the Vector DB
   1) Read, divide and parse documents
   2) Transform Sentence Embedding in the form of Desne Vector
   3) Index to quickly search and use in Retrieval stage and store in Vector DB

2) Input Receive: begins by receiving an input query or prompt, and this query activates the RAG process

3) Search for documents
   1) Enable search system: This system searches for a large document corpus or database (such as Wikipedia or specialized data)
   2) Query processing: Input queries are processed to form search queries that the search system can understand
   3) Import documents: Search system searches corpus based on processed queries and searches for related documents or information snippets

4) Information augmentation
   1) Retrieved documents are used to augment input queries.
   2) Augmented inputs are formed and include both the original query and the additional context of the retrieved document

5) Language models generate answers
   1) Augmented inputs are typically fed to seq2seq language models such as GPT or BART
   2) Response generation: The language model processes augmented inputs and generates responses. This response is informed by external information taken from the search phase as well as the knowledge the model has trained in advance

6) Output generation
   1) Refined or formalized to meet the needs of the application
   2) Delivering the output

7) Feedback loop(optional): In some implementations, there may be a feedback mechanism to evaluate the effectiveness of the retrieved document and the quality of the final output.

## RAG Implementation with LangChain
**Langchain** is a framework designed to simplify application generation using LLM.

* **RAG Pipeline**

![image](https://github.com/hongeunhee/RAG/assets/155135155/73185e08-22e2-4972-bef9-af18e60bbe4a)

1) Create a RunnableParallel object with two entries in the first step
    1) The first item, context, contains document results imported from the searcher
    2) The second item, question, copies it using RunnablePassThrough to convey the question to the user's original question

2) Feed the Dictionary to the prompt component. Configure the prompt using the user input question and the searched document context, and output PromptValue

3) The model component takes the generated prompts and delivers them to the OpenAI LLM model (ChatModel) for evaluation. The output generated by the model is a ChatMessage object

4) Finally, the output_parser component takes a ChatMessage and converts it into a Python string (StrOutputParser), which is returned in the chain.invoke() method



Currently, there are examples of asking questions and answers about candidates' resumes based on langchain 
and examples of requesting analysis of structured data.


* GO TO [the pdf-based Question-Anaswering(Q-A) with LangChain example](https://github.com/hongeunhee/RAG/blob/main/pdf_Q_A_RAG.ipynb)

* GO TO [Request data analysis](https://github.com/hongeunhee/RAG/blob/main/Automating_Data_Analysis_with_Langchain.ipynb)
