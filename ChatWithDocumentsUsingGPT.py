# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os  # íŒŒì¼ ì‘ì—…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tempfile  # ì„ì‹œ íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st  # Streamlitì„ ì‚¬ìš©í•´ ì›¹ ì•± êµ¬ì¶•
from langchain.chat_models import ChatOpenAI  # OpenAI ì±„íŒ… ëª¨ë¸ ì‚¬ìš©, + pip install --upgrade langchain openai
from langchain.document_loaders import PyPDFLoader  # PDF ë¬¸ì„œ ë¡œë“œ
from langchain.memory import ConversationBufferMemory  # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬
from langchain.memory.chat_message_histories import StreamlitChatMessageHistory  # Streamlitì—ì„œ ë©”ì‹œì§€ ì €ì¥
from langchain.embeddings import HuggingFaceEmbeddings  # í…ìŠ¤íŠ¸ ì„ë² ë”© (Hugging Face ëª¨ë¸ ì‚¬ìš©)
from langchain.callbacks.base import BaseCallbackHandler  # ì²´ì¸ì˜ ì‘ì—… ì¤‘ ì½œë°± í•¸ë“¤ë§
from langchain.chains import ConversationalRetrievalChain  # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸
from langchain.vectorstores import DocArrayInMemorySearch  # ë©”ëª¨ë¦¬ ë‚´ ë¬¸ì„œ ê²€ìƒ‰
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="LangChain: Chat with Documents", page_icon="ğŸ¦œ")  # í˜ì´ì§€ ì œëª©ê³¼ ì•„ì´ì½˜ ì„¤ì •
st.title("ğŸ¦œ LangChain: Chat with Documents")  # í˜ì´ì§€ ì œëª© í‘œì‹œ

# ë¬¸ì„œ ê²€ìƒ‰ê¸° ì„¤ì • í•¨ìˆ˜
@st.cache_resource(ttl="1h")  # 1ì‹œê°„ ë™ì•ˆ ê²€ìƒ‰ê¸°ë¥¼ ìºì‹œí•˜ì—¬ ì¬ê³„ì‚° ë°©ì§€
def configure_retriever(uploaded_files):
    # ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ì½ê³  ì²˜ë¦¬í•˜ê¸°
    docs = []  # ë¡œë“œëœ ë¬¸ì„œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    temp_dir = tempfile.TemporaryDirectory()  # ì„ì‹œ ë””ë ‰í„°ë¦¬ ìƒì„±
    for file in uploaded_files:  # ì—…ë¡œë“œëœ íŒŒì¼ë§ˆë‹¤ ë°˜ë³µ
        temp_filepath = os.path.join(temp_dir.name, file.name)  # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
        with open(temp_filepath, "wb") as f:  # íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
            f.write(file.getvalue())
        loader = PyPDFLoader(temp_filepath)  # PDF íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë¡œë” ìƒì„±
        docs.extend(loader.load())  # ë¡œë“œëœ ë‚´ìš©ì„ docs ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

    # ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  # í¬ê¸°ì™€ ì¤‘ë³µ ì„¤ì •
    splits = text_splitter.split_documents(docs)  # ë¬¸ì„œë“¤ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë¶„í• 

    # HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•´ ì„ë² ë”© ìƒì„±
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")  # íŠ¹ì • HuggingFace ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)  # ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DB ìƒì„±
       # ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ ì •ì˜
    retriever = vectordb.as_retriever(search_type="mmr", search_kwargs={"k": 2, "fetch_k": 4})  # ê²€ìƒ‰ ì „ëµ ì„¤ì •

    return retriever  # ì„¤ì •ëœ ë¦¬íŠ¸ë¦¬ë²„ ë°˜í™˜


# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ Streamlitìœ¼ë¡œ ë³´ë‚´ëŠ” ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = ""):
        self.container = container  # ê²°ê³¼ë¥¼ í‘œì‹œí•  Streamlit ì»¨í…Œì´ë„ˆ
        self.text = initial_text  # ì»¨í…Œì´ë„ˆì— í‘œì‹œí•  ì´ˆê¸° í…ìŠ¤íŠ¸
        self.run_id_ignore_token = None  # íŠ¹ì • ì‹¤í–‰ì„ ë¬´ì‹œí•  í† í°

    # LLM ì‹œì‘ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):
        if prompts[0].startswith("Human"):  # ì‚¬ëŒì´ ì…ë ¥í•œ ì§ˆë¬¸ì„ ì¶œë ¥í•˜ì§€ ì•Šê¸° ìœ„í•´
            self.run_id_ignore_token = kwargs.get("run_id")  # ì‹¤í–‰ IDë¥¼ ì €ì¥í•˜ì—¬ íŠ¹ì • ì‹¤í–‰ì„ ë¬´ì‹œ

    # LLMì´ ìƒˆë¡œìš´ í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        if self.run_id_ignore_token == kwargs.get("run_id", False):  # íŠ¹ì • ì‹¤í–‰ì„ ë¬´ì‹œ
            return
        self.text += token  # í† í°ì„ í…ìŠ¤íŠ¸ì— ì¶”ê°€
        self.container.markdown(self.text)  # Streamlit ì»¨í…Œì´ë„ˆì— ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸


# ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ìƒíƒœë¥¼ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬
class PrintRetrievalHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.status = container.status("**Context Retrieval**")  # Streamlitì— ìƒíƒœ í‘œì‹œ ì„¤ì •

    # ë¦¬íŠ¸ë¦¬ë²„ê°€ ì‘ë™ì„ ì‹œì‘í•  ë•Œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
    def on_retriever_start(self, serialized: dict, query: str, **kwargs):
        self.status.write(f"**Question:** {query}")  # ì²˜ë¦¬ ì¤‘ì¸ ì§ˆë¬¸ í‘œì‹œ
        self.status.update(label=f"**Context Retrieval:** {query}")  # ìƒíƒœ ë ˆì´ë¸” ì—…ë°ì´íŠ¸

    # ë¦¬íŠ¸ë¦¬ë²„ê°€ ì‘ì—…ì„ ì™„ë£Œí•  ë•Œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
    def on_retriever_end(self, documents, **kwargs):
        for idx, doc in enumerate(documents):  # ê²€ìƒ‰ëœ ê° ë¬¸ì„œì— ëŒ€í•´ ë°˜ë³µ
            source = os.path.basename(doc.metadata["source"])  # ë¬¸ì„œì˜ ì¶œì²˜(íŒŒì¼ ì´ë¦„) ê°€ì ¸ì˜¤ê¸°
            self.status.write(f"**Document {idx} from {source}**")  # í‘œì‹œë˜ëŠ” ë¬¸ì„œì— ëŒ€í•´ ì•Œë¦¼
            self.status.markdown(doc.page_content)  # ë¬¸ì„œ ë‚´ìš© í‘œì‹œ
        self.status.update(state="complete")  # ê²€ìƒ‰ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œ


# OpenAI API í‚¤ë¥¼ Streamlit ì‚¬ì´ë“œë°”ì—ì„œ ì½ê¸°
openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")  # API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì…ë ¥
if not openai_api_key:  # API í‚¤ê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´
    st.info("Please add your OpenAI API key to continue.")  # API í‚¤ ì…ë ¥ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€ í‘œì‹œ
    st.stop()  # API í‚¤ê°€ ì—†ìœ¼ë©´ ì‹¤í–‰ ì¤‘ë‹¨

# PDF íŒŒì¼ ì—…ë¡œë“œ
uploaded_files = st.sidebar.file_uploader(
    label="Upload PDF files", type=["pdf"], accept_multiple_files=True
)  # ì—¬ëŸ¬ ê°œì˜ PDF íŒŒì¼ ì—…ë¡œë“œ í—ˆìš©
if not uploaded_files:  # íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šìœ¼ë©´
    st.info("Please upload PDF documents to continue.")  # PDF íŒŒì¼ì„ ì—…ë¡œë“œí•  ê²ƒì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€ í‘œì‹œ
    st.stop()  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‹¤í–‰ ì¤‘ë‹¨

# ì—…ë¡œë“œëœ íŒŒì¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰ê¸° ì„¤ì •
retriever = configure_retriever(uploaded_files)

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë©”ëª¨ë¦¬ ì„¤ì •
msgs = StreamlitChatMessageHistory()  # ë©”ì‹œì§€ ê¸°ë¡ì„ ì €ì¥í•  ê°ì²´ ìƒì„±
memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=msgs, return_messages=True)  # ëŒ€í™” ê¸°ë¡ìš© ë©”ëª¨ë¦¬ ì„¤ì •

# ì±„íŒ…ì„ ìœ„í•œ ì–¸ì–´ ëª¨ë¸(LLM) ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo", openai_api_key=openai_api_key, temperature=0, streaming=True
)  # ì±„íŒ…ì„ ìœ„í•œ GPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
qa_chain = ConversationalRetrievalChain.from_llm(
    llm, retriever=retriever, memory=memory, verbose=True
)  # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ì„¤ì •

# ë©”ì‹œì§€ ê¸°ë¡ì„ ì´ˆê¸°í™”í•˜ë ¤ë©´
if len(msgs.messages) == 0 or st.sidebar.button("Clear message history"):  # ë©”ì‹œì§€ê°€ ì—†ê±°ë‚˜ ë²„íŠ¼ í´ë¦­ ì‹œ
    msgs.clear()  # ë©”ì‹œì§€ ê¸°ë¡ ì´ˆê¸°í™”
    msgs.add_ai_message("How can I help you?")  # ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì— ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
avatars = {"human": "user", "ai": "assistant"}  # ì‚¬ìš©ìì™€ AIì˜ ì•„ë°”íƒ€ ì„¤ì •
for msg in msgs.messages:  # ë©”ì‹œì§€ ê¸°ë¡ì„ ë°˜ë³µí•˜ë©´ì„œ
    st.chat_message(avatars[msg.type]).write(msg.content)  # í•´ë‹¹í•˜ëŠ” ì•„ë°”íƒ€ë¡œ ë©”ì‹œì§€ í‘œì‹œ

# ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µ ìƒì„±
if user_query := st.chat_input(placeholder="Ask me anything!"):  # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
    st.chat_message("user").write(user_query)  # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ

    with st.chat_message("assistant"):  # AI ì‘ë‹µ í‘œì‹œ
        retrieval_handler = PrintRetrievalHandler(st.container())  # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¸ë“¤ëŸ¬ ì„¤ì •
        stream_handler = StreamHandler(st.empty())  # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])  # QA ì²´ì¸ ì‹¤í–‰