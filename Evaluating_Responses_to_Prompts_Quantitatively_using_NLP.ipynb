{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOulzmbmtu2Te8T18aA1Fep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongeunhee/RAG/blob/main/Evaluating_Responses_to_Prompts_Quantitatively_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Responses to Prompts Quantitatively Using Natural Language Processing Models**\n",
        "1. Data Preparation:\n",
        "Prepare example data according to evaluation criteria. This data should include various response examples scored for each evaluation criterion.\n",
        "\n",
        "2. Model Training:\n",
        "Train an NLP model using the prepared data. Models like BERT or GPT-4o can be used to evaluate the quality of text. The model learns how to predict scores for each criterion to assess response quality.\n",
        "\n",
        "3. Model Evaluation:\n",
        "The model predicts scores for each evaluation criterion for new responses. Based on these predicted scores, calculate a total score to quantitatively evaluate response quality."
      ],
      "metadata": {
        "id": "VYv5yS6vxd7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation Steps**\n",
        "1. Collection and Labeling of Example Data for Evaluation Criteria:\n",
        "Label example responses for each evaluation criterion (naturalness of conversation, relevance to questions, appropriateness of background setting, clarity).\n",
        "\n",
        "2. Model Training:\n",
        "Train classification or regression models for each criterion using the labeled data. For instance, a text classification model might be used to evaluate appropriateness of background setting.\n",
        "\n",
        "3. Model Evaluation and Validation:\n",
        "Evaluate the performance of the trained model using a validation set. Adjust the model if necessary."
      ],
      "metadata": {
        "id": "G4M__ohCxs8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Sample Data\n",
        "data = [{\"response\":\"I am working on various projects as an artificial intelligence researcher.\", \"background\":9, \"clarity\":8, \"naturalness\":9, \"relevance\":9},\n",
        "{\"response\":\"I used to be an artist in the past, and now I live with my family.\", \"background\":8, \"clarity\":7, \"naturalness\":8, \"relevance\":8}] # 더 많은 데이터 필요\n",
        "\n",
        "# Evaluation Model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define Evaluation Items\n",
        "labels = [\"background\", \"clarity\", \"naturalness\", \"relevance\"]\n",
        "\n",
        "# Assessment of new responses\n",
        "new_response = \"I am working with various clothing brands as a fashion designer.\"\n",
        "result = classifier(new_response, labels)\n",
        "\n",
        "# Score Prediction and Summation\n",
        "predicted_scores = {label: score for label, score in zip(result[\"labels\"], result[\"scores\"])}\n",
        "total_score = sum(predicted_scores.values())\n",
        "\n",
        "print(f\"Predicted Score: {predicted_scores}\")\n",
        "print(f\"Total Score: {total_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsLlCRbNx3cp",
        "outputId": "a82a448b-564c-434a-cf92-5c1b21f531c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Score: {'relevance': 0.3946623206138611, 'clarity': 0.3609298765659332, 'background': 0.1632412075996399, 'naturalness': 0.08116655796766281}\n",
            "Total Score: 0.999999962747097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training regression model using labeled data\n",
        "\n",
        "You can repeat the following steps for each evaluation criterion to train respective regression models."
      ],
      "metadata": {
        "id": "0c_ghQdk17eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = {'response':[\"I am working on various projects as an artificial intelligence researcher.\",\"I used to be an artist in the past, and now I live with my family.\"],\n",
        "        'background_score':[9, 8],\n",
        "        'clarity_score': [8, 7],\n",
        "        'naturalness_score': [9, 8],\n",
        "        'relevance_score': [9, 8]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['response'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['background_score'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Predicted Score: {y_pred[0]}')\n",
        "print(f'Mean Squared Error: {mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTnqiHekx-Mj",
        "outputId": "14ec7cc9-eb9e-43d2-df5e-eab3203718b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Score: 9.0\n",
            "Mean Squared Error: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using BERT embedding"
      ],
      "metadata": {
        "id": "WLd3WAFH28tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Compute BERT embeddings for each response\n",
        "embeddings = np.vstack([get_bert_embeddings(text) for text in df['response']])\n",
        "\n",
        "# Now `embeddings` contains the BERT embeddings for each response\n",
        "# You can proceed to train regression models for each score (background, clarity, naturalness, relevance)\n",
        "\n",
        "# For demonstration, let's assume you want to train a model for `background_score`\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Example for background_score\n",
        "X = embeddings  # BERT embeddings as features\n",
        "y = df['background_score']  # Target variable (background_score)\n",
        "\n",
        "# Train regression model\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(X, y)\n",
        "\n",
        "# After fitting the model, you can use it to predict scores for new responses\n",
        "# For example:\n",
        "new_responses = [\"I am now exploring new opportunities in the tech industry.\",\n",
        "                 \"I have a strong background in mathematics and computer science.\"]\n",
        "\n",
        "new_embeddings = np.vstack([get_bert_embeddings(text) for text in new_responses])\n",
        "predicted_scores = regression_model.predict(new_embeddings)\n",
        "\n",
        "print(\"Predicted background scores for new responses:\", predicted_scores[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5q2goQf1Nw8",
        "outputId": "ee3d5df7-f1ec-4a7c-d329-db8b21087dc8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted background scores for new responses: 8.70787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TilVan87Q2u",
        "outputId": "4e9b9a07-2a2b-4438-8405-ccf333f40948"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.13358934,  0.30654487, -0.2784769 , ..., -0.24665129,\n",
              "        -0.12976412,  0.06576177],\n",
              "       [ 0.23459582,  0.16098467, -0.08742481, ..., -0.2309084 ,\n",
              "         0.2133836 ,  0.02201337]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}